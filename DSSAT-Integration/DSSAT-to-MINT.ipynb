{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSAT Data to MINT Data Catalog\n",
    "This notebook is used to register MINT inputs and data with the MINT Data Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites: python 3.6 or later\n",
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "import pprint\n",
    "import datetime\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "import re\n",
    "\n",
    "# using rdflib to traverse the GSN ontology\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a convenience method to handle api responses. The main portion of the notebook starts in the the next cell\n",
    "def handle_api_response(response, print_response=False):\n",
    "    parsed_response = response.json()\n",
    "\n",
    "    if print_response:\n",
    "        pp.pprint({\"API Response\": parsed_response})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return parsed_response\n",
    "    elif response.status_code == 400:\n",
    "        raise Exception(\"Bad request ^\")\n",
    "    elif response.status_code == 403:\n",
    "        msg = \"Please make sure your request headers include X-Api-Key and that you are using correct url\"\n",
    "        raise Exception(msg)\n",
    "    else:\n",
    "        now = datetime.datetime.utcnow().replace(microsecond=0).isoformat()\n",
    "        msg = f\"\"\"\\n\\n\n",
    "        ------------------------------------- BEGIN ERROR MESSAGE -----------------------------------------\n",
    "    \n",
    "        Automatically generated summary:\n",
    "        - Time of occurrence: {now}\n",
    "        - Request method + url: {response.request.method} - {response.request.url}\n",
    "        - Request headers: {response.request.headers}\n",
    "        - Request body: {response.request.body}\n",
    "        - Response: {parsed_response}\n",
    "\n",
    "        --------------------------------------- END ERROR MESSAGE ------------------------------------------\n",
    "        \\n\\n\n",
    "        \"\"\"\n",
    "        raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real interactions with the data catalog, use api.mint-data-catalog.org\n",
    "url = \"https://sandbox.mint-data-catalog.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you register datasets or resources, we require you to pass a \"provenance_id\". This a unique id associated\n",
    "# with your account so that we can keep track of who is adding things to the data catalog. For sandboxed interactions\n",
    "# with the data catalog api, please use this provenance_id:\n",
    "provenance_id = \"e8287ea4-e6f2-47aa-8bfc-0c22852735c8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X-Api-Key': 'mint-data-catalog:86e48b14-6478-4a57-bacc-2ac0f892a9a9:2a4287bb-5989-49bd-bca5-f9f80a468094'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get session token to use the API\n",
    "resp = requests.get(f\"{url}/get_session_token\").json()\n",
    "print(resp)\n",
    "api_key = resp['X-Api-Key']\n",
    "\n",
    "request_headers = {\n",
    "    'Content-Type': \"application/json\",\n",
    "    'X-Api-Key': api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering `historical_nbg_maiz.json`\n",
    "We can start with the input file for DSSAT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fractionalAW': 0.25,\n",
      " 'incorporationDepth': 5,\n",
      " 'incorporationRate': 100,\n",
      " 'name': 'historical_nbg_maiz',\n",
      " 'plantingDayOfMonth': 1,\n",
      " 'plantingWindow': 45,\n",
      " 'rasters': {'dataLayers': {'elevation': 'rasters/nbg_elevation.tif',\n",
      "                            'harvestedArea': 'rasters/nbg_harvest_maiz.tif',\n",
      "                            'initialNitrogen': 'rasters/nbg_initial_n.tif',\n",
      "                            'plantingMonth': 'rasters/nbg_planting.tif',\n",
      "                            'rootMass': 'rasters/nbg_root_mass.tif',\n",
      "                            'soilProfile': 'rasters/nbg_hc27.tif',\n",
      "                            'soilResidue': 'rasters/nbg_surface_residue.tif',\n",
      "                            'weatherFile': 'rasters/nbg_cellid.tif'}},\n",
      " 'runYears': 33,\n",
      " 'soils': ['base/HC.SOL'],\n",
      " 'startYear': 1984,\n",
      " 'templateDir': 'templates',\n",
      " 'weatherDir': 'weather/nbg',\n",
      " 'workDir': 'out/nbg_maiz/historical',\n",
      " 'wstID': 'SSUD',\n",
      " 'xFileTemplate': 'MAIZ8433.SNX'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(json.loads(open('historical_nbg_maiz.json').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_register = ['plantingWindow',\n",
    "               'plantingDayOfMonth',\n",
    "               'fractionalAW',\n",
    "               'incorporationDepth',\n",
    "               'incorporationRate',\n",
    "               'startYear',\n",
    "               'runYears']\n",
    "\n",
    "to_register_sn = {}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load properties from GSN ontology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N37a8c648b71f45a4b4ff58c4d7e58529 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"svo-properties.rdf\", format=\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9606 properties in GSN\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(g)} properties in GSN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the triples that map to the variables in our input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split('; |, ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year', 'start', 'mass']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('_|-','year-start_mass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triples(search_list, exact=False):\n",
    "    '''\n",
    "    Takes in a list of search terms and returns any triples whose name\n",
    "    contains in any order the terms.\n",
    "    \n",
    "    if `exact=True` then the specific words in `search_list` and only those\n",
    "    words should appear in the name\n",
    "    '''\n",
    "    triples = []\n",
    "    for subj, pred, obj in g:\n",
    "        if '#' in subj:\n",
    "            name = subj.split('#')[1]\n",
    "            if exact:\n",
    "                if set(search_list) == set(re.split('_|-',name.lower())):\n",
    "                    triples.append((subj, pred, obj))\n",
    "            else:\n",
    "                if all(to_search in name.lower() for to_search in search_list):\n",
    "                    triples.append((subj, pred, obj))\n",
    "    for subj,pred,obj in triples:\n",
    "        print(subj)\n",
    "        print(pred)\n",
    "        print(obj)\n",
    "        print()\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all triples associated with `http://www.geoscienceontology.org/svo/svl/property#year`. We can learn:\n",
    "\n",
    "- year's type is a quantitative property\n",
    "- year's type could also be a named individual (?)\n",
    "- year's preferred lable is `year`\n",
    "- year has an associated wikipedia page\n",
    "- year has units `none` (a year is a year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.geoscienceontology.org/svo/svl/property#year\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#QuantitativeProperty\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#year\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#year\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "year\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#year\n",
      "http://www.geoscienceontology.org/svo/svu#hasAssociatedWikipediaPage\n",
      "http://en.wikipedia.org/wiki/Year\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#year\n",
      "http://www.geoscienceontology.org/svo/svu#hasUnits\n",
      "none\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#year'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#QuantitativeProperty')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#year'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#year'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('year', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#year'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#hasAssociatedWikipediaPage'),\n",
       "  rdflib.term.Literal('http://en.wikipedia.org/wiki/Year')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#year'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#hasUnits'),\n",
       "  rdflib.term.Literal('none'))]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_triples(['year'], exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.geoscienceontology.org/svo/svl/property#planting_date\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_depth\n",
      "http://www.geoscienceontology.org/svo/svu#quantifiesProcess\n",
      "http://www.geoscienceontology.org/svo/svl/process#planting\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#Property\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density\n",
      "http://www.geoscienceontology.org/svo/svu#quantifiesProcess\n",
      "http://www.geoscienceontology.org/svo/svl/process#planting\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#QuantitativeProperty\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_date\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "planting_date\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#QuantitativeProperty\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "count-per-area_planting_density\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_date\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#Property\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date\n",
      "http://www.geoscienceontology.org/svo/svu#isTypeOf\n",
      "http://www.geoscienceontology.org/svo/svl/property#date\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_depth\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.geoscienceontology.org/svo/svu#QuantitativeProperty\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "planting_or_sowing_date\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance\n",
      "http://www.geoscienceontology.org/svo/svu#isTypeOf\n",
      "http://www.geoscienceontology.org/svo/svl/property#separation_distance\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_depth\n",
      "http://www.w3.org/1999/02/22-rdf-syntax-ns#type\n",
      "http://www.w3.org/2002/07/owl#NamedIndividual\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density\n",
      "http://www.geoscienceontology.org/svo/svu#isTypeOf\n",
      "http://www.geoscienceontology.org/svo/svl/property#count-per-area_density\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "planting_separation_distance\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_depth\n",
      "http://www.geoscienceontology.org/svo/svu#isTypeOf\n",
      "http://www.geoscienceontology.org/svo/svl/property#depth\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_depth\n",
      "http://www.w3.org/2004/02/skos/core#prefLabel\n",
      "planting_depth\n",
      "\n",
      "http://www.geoscienceontology.org/svo/svl/property#planting_date\n",
      "http://www.geoscienceontology.org/svo/svu#isTypeOf\n",
      "http://www.geoscienceontology.org/svo/svl/property#date\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_depth'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#quantifiesProcess'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/process#planting')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#Property')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#quantifiesProcess'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/process#planting')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#QuantitativeProperty')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('planting_date', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#QuantitativeProperty')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('count-per-area_planting_density', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#Property')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#isTypeOf'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#date')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_depth'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#QuantitativeProperty')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_or_sowing_date'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('planting_or_sowing_date', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#isTypeOf'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#separation_distance')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_depth'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2002/07/owl#NamedIndividual')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_planting_density'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#isTypeOf'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#count-per-area_density')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_separation_distance'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('planting_separation_distance', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_depth'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#isTypeOf'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#depth')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_depth'),\n",
       "  rdflib.term.URIRef('http://www.w3.org/2004/02/skos/core#prefLabel'),\n",
       "  rdflib.term.Literal('planting_depth', lang='en')),\n",
       " (rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#planting_date'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svu#isTypeOf'),\n",
       "  rdflib.term.URIRef('http://www.geoscienceontology.org/svo/svl/property#date'))]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_triples(['planting'], exact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Registering Standard Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param[name] standard variable name (aka label)\n",
    "# @param[ontology] name of the ontology where standard variables are defined\n",
    "# @param[uri] uri of standard variable name (note that this is full uri, which includes the ontology)\n",
    "standard_variable_defs = {\n",
    "    \"standard_variables\": [\n",
    "        {\n",
    "            \"name\": \"Time_Standard_Variable\",\n",
    "            \"ontology\": \"MyOntology\",\n",
    "            \"uri\": \"http://my_ontology_uri.org/standard_names/time_standard_variable\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Temperature_Standard_Variable\",\n",
    "            \"ontology\": \"MyOntology\",\n",
    "            \"uri\": \"http://my_ontology_uri.org/standard_names/temperature_standard_variable\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/knowledge_graph/register_standard_variables\", \n",
    "                    headers=request_headers, \n",
    "                    json=standard_variable_defs)\n",
    "\n",
    "\n",
    "# If request is successful, it will return 'result': 'success' along with a list of registered standard variables\n",
    "# and their record_ids. Those record_ids are unique identifiers (UUID) and you will need them down the road to \n",
    "# register variables\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "records = parsed_response['standard_variables']\n",
    "\n",
    "# iterate through the list of returned standard variable objects and save\n",
    "# the ones whose names match the one that we want and store them in python variables\n",
    "time_standard_variable = next(record for record in records if record[\"name\"] == \"Time_Standard_Variable\")\n",
    "temperature_standard_variable = next(record for record in records if record[\"name\"] == \"Temperature_Standard_Variable\")\n",
    "\n",
    "## Uncomment below to see the structure of a specific variable:\n",
    "# pp.pprint({\"Time Standard Variable\": time_standard_variable})\n",
    "# pp.pprint({\"Temperature Standard Variable\": temperature_standard_variable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to check if specific standard variables have already been registered in the data catalog, \n",
    "# you can search by name and data catalog will return existing records.\n",
    "nonexistent_name = str(uuid.uuid4())\n",
    "print(f\"This name does not exist: {nonexistent_name}\")\n",
    "\n",
    "search_query = {\n",
    "    \"name__in\": [\"Time_Standard_Variable\", \"Temperature_Standard_Variable\", nonexistent_name]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/knowledge_graph/find_standard_variables\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=search_query)\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "# Below is how you'd extract standard_variables from the response if you need to reference them (their record_ids)\n",
    "# later:\n",
    "# \n",
    "# existing_standard_variables = parsed_response[\"standard_variables\"]\n",
    "# print(existing_standard_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After you are satisfied that all relevant standard variables are in the data catalog (usually it's a one-time thing), you can proceed to register datasets, variables, and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: REGISTER DATASETS\n",
    "\n",
    "#### FAQ: How do I know what is a dataset and what is a resource?\n",
    "There isn't a cut-and-dry answer and will utimately depend on the way your organize\n",
    "and think about your data. We define a dataset as \"logical grouping of variables in a collection\n",
    "of resources\". As long as your way fits this extremely broad definition, you should be ok. \n",
    "To illustrate this, let's go back to our toy example of *\"Temperature recorded outside my house\"*,\n",
    "for which I record data for my time and temperature variables in multiple files. Originally, \n",
    "each file was a resource under my dataset, which makes sense because all of these data files\n",
    "describe (semantically) the same concept - temperature recorded outside my house. On the other\n",
    "hand, I could've made a similarly strong argument that each file is actually a separate\n",
    "logical entity that provides temperature data recorded outside my house on a *specific* date\n",
    "and should therefore be semantically differentiated from the temperature recorded on another date.\n",
    "But in vast majority of cases, this distinction doesn't really matter because in the end, those that \n",
    "care about knowing the temperature outside my house on Jan 1st 2018 will be able to find \n",
    "the link to that file and download it. From the perspective of the end user, they care about the actual\n",
    "raw data, and not the (arguably somewhat arbitrary) distinction between a resource and a dataset. There will\n",
    "be an example later on how to \"tag\" your data with relevant temporal/spatial information so that it becomes\n",
    "searchable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"4e8ade31-7729-4891-a462-2dac66158512\" # This is optional; if not given, it will be auto-generated\n",
    "\n",
    "## An example of how to generate a random uuid yourself (will be different every time method is run)\n",
    "# print(str(uuid.uuid4()))\n",
    "# print(str(uuid.uuid4()))\n",
    "#\n",
    "## This will generate the same record_id as long as the input string remains the same\n",
    "#\n",
    "# input_string = \"some string 34_\"\n",
    "# print(str(uuid.uuid5(uuid.NAMESPACE_URL, str(input_string))))\n",
    "# print(str(uuid.uuid5(uuid.NAMESPACE_URL, str(input_string))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note about ids and record_ids. \n",
    "Every entity in data catalog (variables, standard_variables, \n",
    "datasets, resources) will have a unique id/record_id associated with it. This is what disambiguates\n",
    "e.g., two datasets that are named \"MyDataset\". These record_ids are either generated automatically,\n",
    "on our end if no \"record_id\" is provided, or you can generate them yourself using Python's [uuid](https://docs.python.org/3/library/uuid.html)\n",
    "library (or any other library that generates uuids according to the international standard).\n",
    "\n",
    "What this means in practice is that if you remove \"record_id\" from the dictionary below\n",
    "and rerun this cell 3 times, you will end up registering 3 datasets with identical name, description,\n",
    "metadata, and provenance_id. This is why if you register a new dataset (or variable, or resource, etc), \n",
    "it's important to note returned object's record_id if/when you need to reference it later, \n",
    "rerun the same script in an indempotent manner, or update record's attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build datasets definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_defs = {\n",
    "    \"datasets\": [\n",
    "        {\n",
    "            \"record_id\": dataset_id, # Remove this line if you want to create a new dataset\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"metadata\": {\n",
    "                \"any_additional_metadata\": \"content\"\n",
    "            },\n",
    "            \"description\": \"Temperature recorded outside my house; collected over last month\",\n",
    "            \"name\": \"Temperature recorded outside my house\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/register_datasets\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=dataset_defs)\n",
    "\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "datasets = parsed_response[\"datasets\"]\n",
    "\n",
    "# Iterate through the list of returned datasets objects and save the one whose name matches our name \n",
    "# to a Python variable\n",
    "dataset_record = next(record for record in datasets if record[\"name\"] == \"Temperature recorded outside my house\")\n",
    "# Extract dataset record_id and store it in a variable\n",
    "dataset_record_id = dataset_record[\"record_id\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Register variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, these ids are optional and will be auto-generated if not given. They are included here in order\n",
    "# to make requests indempotent (so that new records aren't beeing generated every time this code block is run)\n",
    "\n",
    "time_variable_record_id = '9358af57-192f-4cc3-9bee-837e76819674'\n",
    "temperature_variable_record_id = 'c22deb3b-ebda-48cb-950a-2f4f00498197'\n",
    "\n",
    "variable_defs = {\n",
    "    \"variables\": [\n",
    "        {\n",
    "            \"record_id\": time_variable_record_id, # If you remove this line, record_id will be auto-generated\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"name\": \"Time\",\n",
    "            \"metadata\": {\n",
    "                \"units\": \"ISO8601_datetime\"\n",
    "                # Can include any other metadata that you want to associate with the variable\n",
    "            },\n",
    "            \"standard_variable_ids\": [\n",
    "                # Recall that we created \"time_standard_variable\" python object after\n",
    "                # registering our standard variables. We just need its unique identifier - \n",
    "                # record_id - in order to associate it with our \"Time\" variable. Also, note \n",
    "                # that \"standard_variable_ids\" is an array, so you can associate multiple\n",
    "                # standard variables with our \"local\" variable (and it does not have\n",
    "                # to be done all at once). That is how we can semantically link multiple\n",
    "                # standard names and ontologies later on\n",
    "                time_standard_variable[\"record_id\"]\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": temperature_variable_record_id, # If you remove this line, record_id will be auto-generated\n",
    "            \"dataset_id\": dataset_record_id, # from register_datasets() call\n",
    "            \"name\": \"Temperature\",\n",
    "            \"metadata\": {\n",
    "                \"units\": \"F\"\n",
    "            },\n",
    "            \"standard_variable_ids\": [\n",
    "                temperature_standard_variable[\"record_id\"]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/register_variables\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=variable_defs)\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "variables = parsed_response[\"variables\"]\n",
    "\n",
    "time_variable = next(record for record in variables if record[\"name\"] == \"Time\")\n",
    "temperature_variable = next(record for record in variables if record[\"name\"] == \"Temperature\")\n",
    "\n",
    "## Uncomment below to print individual records\n",
    "# print(f\"Time Variable: {time_variable}\")\n",
    "# print(f\"Temperature Variable: {temperature_variable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that I host my datasets files on www.my_domain.com/storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage_url = \"www.my_domain.com/storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, assume that I've collected 2 days worth of data in temp_records_2018_01_01.csv and temp_records_2018_01_02.csv ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_name = \"temp_records_2018_01_01.csv\"\n",
    "file_2_name = \"temp_records_2018_01_02.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and uploaded them to my remote storage location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_data_url = f\"{data_storage_url}/{file_1_name}\"\n",
    "file_2_data_url = f\"{data_storage_url}/{file_2_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to dataset and variable registrations, we are going to generate unique resource record_ids to \n",
    "# make these requests repeatable without creating new records. But remember, these will be auto-generated\n",
    "# if not given\n",
    "\n",
    "file_1_record_id = \"dd52e66b-3149-4d46-8f8e-a18e46136e55\"\n",
    "file_2_record_id = \"25916ccf-d108-4187-b243-2b257ce67fa5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making my files searchable by time\n",
    "\n",
    "If I want my resources to be searchable by time range, I can \"annotate\" each resource with corresponding \n",
    "temporal coverage. That way, when someone searches for any datasets that contain \"Temperature_Standard_Variable\"\n",
    "for January 01 2018, my file_1_name will be returned, along with the data url, and the users will be able to \n",
    "download it easily. Note that temporal coverage must have \"start_time\" and \"end_time\" and must follow ISO 8601 \n",
    "datetime format YYYY-MM-DDTHH:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_temporal_coverage = {\n",
    "    \"start_time\": \"2018-01-01T00:00:00\",\n",
    "    \"end_time\": \"2018-01-01T23:59:59\"\n",
    "}\n",
    "file_2_temporal_coverage = {\n",
    "    \"start_time\": \"2018-01-02T00:00:00\",\n",
    "    \"end_time\": \"2018-01-02T23:59:59\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making my files spatially searchable \n",
    "\n",
    "Let's say that my house is somewhere in LA, defined by the following bounding box \n",
    "(where x refers to longitude and y refers to latitude)  \n",
    "x_min:  33.9605286  \n",
    "y_min: -118.4253354  \n",
    "x_max: 33.9895077  \n",
    "y_max: -118.4093589  \n",
    "\n",
    "We can annotate our resources with spatial coverage. Since all of our resources come from the same location, we can reuse the same values. If you have multiple resources with with different locations, you can follow temporal annotation example above.\n",
    "\n",
    "Things to note here are the required \"type\" and \"value\" parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_coverage = {\n",
    "    \"type\": \"BoundingBox\",\n",
    "    \"value\": {\n",
    "        \"xmin\": 33.9605286,\n",
    "        \"ymin\": -118.4253354,\n",
    "        \"xmax\": 33.9895077,\n",
    "        \"ymax\": -118.4093589\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we can build our resource definitions and register them (in bulk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_defs = {\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"record_id\": file_1_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                temperature_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_1_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_1_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": spatial_coverage,\n",
    "                \"temporal_coverage\": file_1_temporal_coverage\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        },\n",
    "        {\n",
    "            \"record_id\": file_2_record_id,\n",
    "            \"dataset_id\": dataset_record_id,\n",
    "            \"provenance_id\": provenance_id,\n",
    "            \"variable_ids\": [\n",
    "                time_variable[\"record_id\"],\n",
    "                temperature_variable[\"record_id\"]\n",
    "            ],\n",
    "            \"name\": file_2_name,\n",
    "            \"resource_type\": \"csv\",\n",
    "            \"data_url\": file_2_data_url,\n",
    "            \"metadata\": {\n",
    "                \"spatial_coverage\": spatial_coverage,\n",
    "                \"temporal_coverage\": file_2_temporal_coverage\n",
    "            },\n",
    "            \"layout\": {}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ... and register them in bulk\n",
    "resp = requests.post(f\"{url}/datasets/register_resources\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=resource_defs)\n",
    "\n",
    "\n",
    "parsed_response = handle_api_response(resp, print_response=True)\n",
    "\n",
    "\n",
    "resources = parsed_response[\"resources\"]\n",
    "    \n",
    "resource_1 = next(record for record in resources if record[\"name\"] == file_1_name)\n",
    "resource_2 = next(record for record in resources if record[\"name\"] == file_2_name)\n",
    "\n",
    "## Uncomment below to print individual records    \n",
    "# print(f\"{file_1_name}: {resource_1}\")\n",
    "# print(f\"{file_2_name}: {resource_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for datasets/resources\n",
    "\n",
    "After registering datasets/variables/resources, we can now programmatically search of relevant information. Below,\n",
    "you'll see 3 examples of searching for data using standard variable names, temporal, and spatial coverages. Currently, these are the only search filters we support, but we'll be adding more as we get more feature requests.\n",
    "If you would like to search data catalog by other keywords, please let me know at danf@usc.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Searching by standard_names\n",
    "\n",
    "search_query_1 = {\n",
    "    \"standard_variable_names__in\": [temperature_standard_variable[\"name\"]]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/find\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=search_query_1).json()\n",
    "if resp['result'] == 'success':\n",
    "    found_resources = resp['resources']\n",
    "    print(f\"Found {len(found_resources)} resources\")\n",
    "    print(found_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Searching by spatial_coverage\n",
    "\n",
    "# Bounding box search parameter is a 4-element numeric array (in WGS84 coordinate system) [xmin, ymin, xmax, ymax]\n",
    "# As a reminder, x is longitude, y is latitude\n",
    "bounding_box = [\n",
    "    spatial_coverage[\"value\"][\"xmin\"], \n",
    "    spatial_coverage[\"value\"][\"ymin\"], \n",
    "    spatial_coverage[\"value\"][\"xmax\"],\n",
    "    spatial_coverage[\"value\"][\"ymax\"]\n",
    "]\n",
    "\n",
    "search_query_2 = {\n",
    "    \"spatial_coverage__within\": bounding_box\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/find\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=search_query_2).json()\n",
    "if resp['result'] == 'success':\n",
    "    found_resources = resp['resources']\n",
    "    print(f\"Found {len(found_resources)} resources\")\n",
    "    print(found_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Searching by temporal_coverage and standard_names\n",
    "\n",
    "# Bounding box search parameter is a 4-element numeric array (in WGS84 coordinate system) [xmin, ymin, xmax, ymax]\n",
    "# As a reminder, x is longitude, y is latitude\n",
    "start_time = \"2018-01-01T00:00:00\"\n",
    "end_time = \"2018-01-21T23:59:59\"\n",
    "\n",
    "search_query_3 = {\n",
    "    \"standard_variable_names__in\": [temperature_standard_variable[\"name\"]],\n",
    "    \"start_time__gte\": start_time,\n",
    "    \"end_time__lte\": end_time\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/find\", \n",
    "                                        headers=request_headers,\n",
    "                                        json=search_query_3).json()\n",
    "\n",
    "if resp['result'] == 'success':\n",
    "    found_resources = resp['resources']\n",
    "    print(f\"Found {len(found_resources)} resources\")\n",
    "    pp.pprint(found_resources)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Searching by dataset_names\n",
    "\n",
    "search_query_4 = {\n",
    "    \"dataset_names__in\": [\"Temperature recorded outside my house\"]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/find\",\n",
    "                     headers=request_headers,\n",
    "                     json=search_query_4).json()\n",
    "\n",
    "if resp['result'] == 'success':\n",
    "    found_resources = resp['resources']\n",
    "    print(f\"Found {len(found_resources)} resources\")\n",
    "    pp.pprint(found_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Searching by dataset ids\n",
    "\n",
    "search_query_5 = {\n",
    "    \"dataset_ids__in\": [dataset_id]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{url}/datasets/find\",\n",
    "                     headers=request_headers,\n",
    "                     json=search_query_5).json()\n",
    "\n",
    "if resp['result'] == 'success':\n",
    "    found_resources = resp['resources']\n",
    "    print(f\"Found {len(found_resources)} resources\")\n",
    "    pp.pprint(found_resources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
